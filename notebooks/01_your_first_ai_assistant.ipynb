{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Your First AI Assistant\n",
    "\n",
    "## AI4Science series: Programming with Large Language Models\n",
    "\n",
    "**Duration**: ~25 minutes\n",
    "\n",
    "**Learning Goals**:\n",
    "- Set up OpenAI API access\n",
    "- Understand prompt engineering basics\n",
    "- Make your first API call\n",
    "- Apply LLMs to real research tasks (literature summarization)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's install the required packages and set up our API access.\n",
    "\n",
    "### 1.1 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this once)\n",
    "!pip install openai pandas -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Configure Your API Key\n",
    "\n",
    "**In Google Colab:**\n",
    "1. Click on the ðŸ”‘ (Key) icon in the left sidebar\n",
    "2. Click \"+ Add a secret\"\n",
    "3. Name: `OPENAI_API_KEY`\n",
    "4. Value: Your OpenAI API key (get one at https://platform.openai.com/api-keys)\n",
    "5. Toggle \"Notebook access\" ON\n",
    "\n",
    "**Running locally?** Set the environment variable: `export OPENAI_API_KEY='your-key-here'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# Try to get API key from Colab secrets, fall back to environment variable\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    api_key = userdata.get('OPENAI_API_KEY')\n",
    "except:\n",
    "    api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"Please set your OPENAI_API_KEY in Colab secrets or environment variables\")\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "print(\"âœ“ OpenAI client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Test Your Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test to verify everything works\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say 'Hello, scientist!' in exactly 3 words.\"}],\n",
    "    max_tokens=10\n",
    ")\n",
    "print(\"API Response:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Understanding the API\n",
    "\n",
    "### 2.1 The Messages Structure\n",
    "\n",
    "LLM conversations use a list of messages with different roles:\n",
    "\n",
    "| Role | Purpose | Example |\n",
    "|------|---------|--------|\n",
    "| `system` | Sets the AI's persona and behavior | \"You are a helpful biology research assistant\" |\n",
    "| `user` | Your questions/requests | \"Explain this abstract in simple terms\" |\n",
    "| `assistant` | The AI's responses | (Previous AI responses in a conversation) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: A conversation with roles\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful research assistant specializing in explaining scientific concepts simply.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is PCR in one sentence?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Temperature: Controlling Creativity\n",
    "\n",
    "The `temperature` parameter (0.0 to 2.0) controls randomness:\n",
    "- **Low (0.0-0.3)**: Deterministic, consistent outputs - good for factual tasks\n",
    "- **Medium (0.5-0.7)**: Balanced - good for most research tasks\n",
    "- **High (0.8-1.0+)**: Creative, varied outputs - good for brainstorming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different temperatures\n",
    "prompt = \"Suggest a creative name for a new protein that regulates sleep.\"\n",
    "\n",
    "print(\"Low temperature (0.2) - More predictable:\")\n",
    "for i in range(3):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2,\n",
    "        max_tokens=20\n",
    "    )\n",
    "    print(f\"  {i+1}. {response.choices[0].message.content}\")\n",
    "\n",
    "print(\"\\nHigh temperature (1.0) - More creative:\")\n",
    "for i in range(3):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=1.0,\n",
    "        max_tokens=20\n",
    "    )\n",
    "    print(f\"  {i+1}. {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tokens: Understanding Costs and Limits\n",
    "\n",
    "LLMs process text in \"tokens\" (roughly 4 characters or 3/4 of a word):\n",
    "- **Input tokens**: What you send to the API\n",
    "- **Output tokens**: What the API returns\n",
    "- **Cost**: You pay per token (input and output priced separately)\n",
    "\n",
    "**Cost reference (gpt-4o-mini, as of 2024):**\n",
    "- Input: ~$0.15 per 1M tokens\n",
    "- Output: ~$0.60 per 1M tokens\n",
    "- A typical abstract summary: ~$0.0001 (fractions of a cent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check token usage\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain mitochondria in 50 words.\"}]\n",
    ")\n",
    "\n",
    "print(\"Response:\", response.choices[0].message.content)\n",
    "print(\"\\n--- Token Usage ---\")\n",
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Your First Prompts\n",
    "\n",
    "Let's create a reusable helper function and explore prompt engineering basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm(system_prompt, user_message, model=\"gpt-4o-mini\", temperature=0.7):\n",
    "    \"\"\"\n",
    "    Simple wrapper for OpenAI API calls.\n",
    "    \n",
    "    Args:\n",
    "        system_prompt: Instructions for how the AI should behave\n",
    "        user_message: Your question or request\n",
    "        model: Which model to use (default: gpt-4o-mini for cost efficiency)\n",
    "        temperature: Creativity level 0-2 (default: 0.7)\n",
    "    \n",
    "    Returns:\n",
    "        The AI's response as a string\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test it\n",
    "result = ask_llm(\n",
    "    \"You are a helpful assistant.\",\n",
    "    \"What is the significance of p < 0.05 in research?\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Simple Question â†’ Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic question\n",
    "result = ask_llm(\n",
    "    \"You are a helpful research assistant.\",\n",
    "    \"What statistical test should I use to compare means of two independent groups?\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Adding Context for Better Answers\n",
    "\n",
    "The more context you provide, the better the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without context\n",
    "result_no_context = ask_llm(\n",
    "    \"You are a helpful assistant.\",\n",
    "    \"What test should I use for my data?\"\n",
    ")\n",
    "print(\"Without context:\")\n",
    "print(result_no_context)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# With context\n",
    "result_with_context = ask_llm(\n",
    "    \"You are a helpful statistics assistant for biology research.\",\n",
    "    \"\"\"I have data from an experiment with the following characteristics:\n",
    "    - Two groups: control (n=15) and treatment (n=18)\n",
    "    - Measuring tumor volume (continuous, in mmÂ³)\n",
    "    - Data appears normally distributed based on Shapiro-Wilk test\n",
    "    - Variances are similar between groups\n",
    "    \n",
    "    What statistical test should I use?\"\"\"\n",
    ")\n",
    "print(\"With context:\")\n",
    "print(result_with_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 The \"Act as an Expert\" Pattern\n",
    "\n",
    "One of the most powerful prompt engineering techniques: tell the AI what expert to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic assistant\n",
    "generic = ask_llm(\n",
    "    \"You are a helpful assistant.\",\n",
    "    \"How should I visualize the relationship between gene expression levels and patient survival?\"\n",
    ")\n",
    "print(\"Generic assistant:\")\n",
    "print(generic[:500] + \"...\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Expert bioinformatician\n",
    "expert = ask_llm(\n",
    "    \"\"\"You are an expert bioinformatician with 15 years of experience in cancer genomics. \n",
    "    You specialize in survival analysis and gene expression studies. \n",
    "    Provide practical, specific advice that can be immediately implemented in R or Python.\"\"\",\n",
    "    \"How should I visualize the relationship between gene expression levels and patient survival?\"\n",
    ")\n",
    "print(\"Expert bioinformatician:\")\n",
    "print(expert[:800] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Practical Exercise: Literature Summary\n",
    "\n",
    "Now let's apply what we've learned to a real research task: summarizing and extracting information from paper abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample abstract (from our data files)\n",
    "abstract = \"\"\"\n",
    "The gut-brain axis has emerged as a critical pathway linking intestinal microbiome \n",
    "composition to neurological and psychiatric outcomes. This prospective cohort study \n",
    "followed 2,847 adults over three years, collecting stool samples and administering \n",
    "validated mental health assessments at six-month intervals. 16S rRNA sequencing revealed \n",
    "that individuals with depression showed significantly reduced abundance of Faecalibacterium \n",
    "and Coprococcus species compared to healthy controls (p<0.001). Longitudinal analysis \n",
    "demonstrated that changes in Lactobacillus abundance preceded changes in anxiety scores \n",
    "by approximately two months. Metabolomic profiling identified reduced short-chain fatty \n",
    "acid production as a potential mechanism. A subset of participants (n=156) underwent a \n",
    "randomized controlled trial of targeted probiotic supplementation, showing modest but \n",
    "significant improvements in depression scores (Cohen's d=0.35). These findings support \n",
    "the microbiome as both a biomarker and potential therapeutic target for mental health disorders.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Task: Summarize in Plain Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = ask_llm(\n",
    "    \"You are a science communicator who explains research to non-specialists.\",\n",
    "    f\"Summarize this abstract in 3 bullet points that a non-scientist could understand:\\n\\n{abstract}\"\n",
    ")\n",
    "print(\"Plain language summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Task: Extract Key Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ask_llm(\n",
    "    \"You are a methodological expert who identifies research techniques.\",\n",
    "    f\"\"\"Extract and list all the methods/techniques mentioned in this abstract.\n",
    "    Format as a bullet list with brief descriptions of what each method does.\n",
    "    \n",
    "    Abstract:\n",
    "    {abstract}\"\"\"\n",
    ")\n",
    "print(\"Methods extracted:\")\n",
    "print(methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Task: Suggest Related Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ask_llm(\n",
    "    \"You are a librarian expert in scientific literature searches.\",\n",
    "    f\"\"\"Based on this abstract, suggest 10 related search keywords or phrases \n",
    "    that would help find similar papers in PubMed or Google Scholar.\n",
    "    Include both specific terms and broader related concepts.\n",
    "    \n",
    "    Abstract:\n",
    "    {abstract}\"\"\"\n",
    ")\n",
    "print(\"Suggested search keywords:\")\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Putting It Together: A Literature Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_abstract(abstract_text):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of a scientific abstract.\n",
    "    Returns a structured analysis including summary, methods, and keywords.\n",
    "    \"\"\"\n",
    "    analysis_prompt = f\"\"\"Analyze this scientific abstract and provide:\n",
    "\n",
    "1. **Plain Language Summary** (2-3 sentences for a general audience)\n",
    "2. **Key Findings** (bullet points of main results)\n",
    "3. **Methods Used** (list the techniques/approaches)\n",
    "4. **Limitations or Gaps** (what the study doesn't address)\n",
    "5. **Search Keywords** (5 terms for finding related papers)\n",
    "\n",
    "Abstract:\n",
    "{abstract_text}\"\"\"\n",
    "    \n",
    "    return ask_llm(\n",
    "        \"You are an expert scientific reviewer with broad interdisciplinary knowledge.\",\n",
    "        analysis_prompt,\n",
    "        temperature=0.5  # Lower temperature for more consistent analysis\n",
    "    )\n",
    "\n",
    "# Test with our abstract\n",
    "analysis = analyze_abstract(abstract)\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Your Turn: Practice Exercises\n",
    "\n",
    "### Exercise 1: Analyze a Different Abstract\n",
    "\n",
    "Try the `analyze_abstract` function on this climate science abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_abstract = \"\"\"\n",
    "Accurate prediction of regional climate change impacts remains challenging due to \n",
    "the complexity of Earth system interactions. We present an ensemble machine learning \n",
    "approach combining gradient boosting, neural networks, and Gaussian processes to \n",
    "improve regional temperature and precipitation projections. Training data comprised \n",
    "150 years of historical observations and outputs from 35 CMIP6 climate models. Our \n",
    "ensemble achieved 23% lower RMSE compared to individual models when validated against \n",
    "held-out observational data. Feature importance analysis revealed that sea surface \n",
    "temperature patterns and atmospheric circulation indices were most predictive of \n",
    "regional outcomes. We applied our models to generate probabilistic projections for \n",
    "agricultural regions in Southeast Asia, identifying areas at highest risk for drought \n",
    "intensification.\n",
    "\"\"\"\n",
    "\n",
    "# YOUR CODE HERE: Use analyze_abstract() on the climate_abstract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Create Your Own Expert Prompt\n",
    "\n",
    "Write a system prompt for an expert in YOUR field and ask them a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Define your expert system prompt\n",
    "my_expert_prompt = \"\"\"You are an expert in [YOUR FIELD] with experience in [SPECIFIC AREAS].\n",
    "You provide practical, evidence-based advice.\"\"\"\n",
    "\n",
    "# Ask your question\n",
    "my_question = \"[YOUR QUESTION HERE]\"\n",
    "\n",
    "# Get the response\n",
    "# response = ask_llm(my_expert_prompt, my_question)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Compare Models\n",
    "\n",
    "Try the same prompt with different models to see the difference in quality and cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"Explain the difference between Type I and Type II errors in hypothesis testing.\"\n",
    "\n",
    "# gpt-4o-mini (faster, cheaper)\n",
    "print(\"=== GPT-4o-mini ===\")\n",
    "response_mini = ask_llm(\"You are a statistics tutor.\", test_prompt, model=\"gpt-4o-mini\")\n",
    "print(response_mini)\n",
    "\n",
    "# Uncomment to try gpt-4o (more capable, more expensive)\n",
    "# print(\"\\n=== GPT-4o ===\")\n",
    "# response_4o = ask_llm(\"You are a statistics tutor.\", test_prompt, model=\"gpt-4o\")\n",
    "# print(response_4o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Key Takeaways\n",
    "\n",
    "### What You've Learned:\n",
    "1. **API Setup**: How to configure and authenticate with OpenAI\n",
    "2. **Messages Structure**: System, user, and assistant roles\n",
    "3. **Temperature**: Control creativity vs consistency\n",
    "4. **Prompt Engineering Basics**:\n",
    "   - Provide context\n",
    "   - Use the \"expert\" pattern\n",
    "   - Be specific about output format\n",
    "\n",
    "### Best Practices for Research Tasks:\n",
    "- Use **low temperature (0.3-0.5)** for factual analysis\n",
    "- Use **medium temperature (0.5-0.7)** for general tasks\n",
    "- Use **higher temperature (0.8+)** only for brainstorming\n",
    "- Always provide relevant **context** about your data/field\n",
    "- Specify the **format** you want (bullet points, paragraphs, JSON, etc.)\n",
    "\n",
    "### Cost-Saving Tips:\n",
    "- Start with **gpt-4o-mini** for most tasks\n",
    "- Upgrade to **gpt-4o** only when needed for complex reasoning\n",
    "- Set **max_tokens** when you only need short responses\n",
    "\n",
    "---\n",
    "\n",
    "## Next: Notebook 2 - Data Visualization with AI\n",
    "\n",
    "In the next notebook, you'll learn to:\n",
    "- Describe your data to an LLM\n",
    "- Generate visualization code from natural language\n",
    "- Iterate on plots through conversation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
