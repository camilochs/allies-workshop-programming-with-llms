{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Automating Repetitive Tasks\n",
    "\n",
    "## LLM Allies for Scientists Workshop\n",
    "\n",
    "**Duration**: ~35 minutes\n",
    "\n",
    "**Learning Goals**:\n",
    "- Use LLMs to generate data processing scripts\n",
    "- Batch process multiple files\n",
    "- Create reusable analysis pipelines\n",
    "- Clean messy data with AI assistance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai pandas numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# API setup\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    api_key = userdata.get('OPENAI_API_KEY')\n",
    "except:\n",
    "    api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"Please set your OPENAI_API_KEY\")\n",
    "\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions from previous notebooks\n",
    "def ask_llm(system_prompt, user_message, model=\"gpt-4o-mini\", temperature=0.7):\n",
    "    \"\"\"Simple wrapper for OpenAI API calls.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def safe_execute(code_string, local_vars=None):\n",
    "    \"\"\"Execute code with error handling.\"\"\"\n",
    "    if local_vars is None:\n",
    "        local_vars = {}\n",
    "    \n",
    "    # Clean markdown formatting\n",
    "    clean_code = code_string.strip()\n",
    "    if clean_code.startswith('```python'):\n",
    "        clean_code = clean_code[9:]\n",
    "    if clean_code.startswith('```'):\n",
    "        clean_code = clean_code[3:]\n",
    "    if clean_code.endswith('```'):\n",
    "        clean_code = clean_code[:-3]\n",
    "    clean_code = clean_code.strip()\n",
    "    \n",
    "    try:\n",
    "        exec(clean_code, globals(), local_vars)\n",
    "        return True, local_vars, None\n",
    "    except Exception as e:\n",
    "        return False, local_vars, str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. From Manual to Automated\n",
    "\n",
    "### The \"Show Me One, Automate the Rest\" Pattern\n",
    "\n",
    "The most powerful way to use LLMs for automation:\n",
    "1. **Do one example manually** (or describe it in detail)\n",
    "2. **Show the LLM what you did**\n",
    "3. **Ask it to generalize into a reusable function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for automation tasks\n",
    "AUTOMATION_SYSTEM_PROMPT = \"\"\"You are an expert Python programmer who specializes in data processing automation.\n",
    "\n",
    "Rules:\n",
    "1. Write clean, well-documented Python code\n",
    "2. Include docstrings with clear parameter descriptions\n",
    "3. Add appropriate error handling\n",
    "4. Use type hints where helpful\n",
    "5. Output only code, no explanations unless asked\n",
    "6. Prefer pandas for data manipulation\n",
    "7. Make functions reusable and generalizable\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Automating a Manual Process\n",
    "\n",
    "Let's say you manually cleaned some concentration values. Now let's automate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create messy data (simulating real-world issues)\n",
    "messy_df = pd.DataFrame({\n",
    "    'sample_id': ['EXP001', 'EXP002', 'EXP003', 'EXP004', 'EXP005', 'EXP006'],\n",
    "    'concentration': ['1.5', '2.0 mM', '1.5mM', 'N/A', '3.0', '2.5 mM'],\n",
    "    'measurement_date': ['2024-01-15', '01/16/2024', 'Jan 17, 2024', '2024-01-18', '2024/01/19', '20-Jan-2024'],\n",
    "    'temperature': ['25C', '25.0', '25 C', 'NA', '25.5°C', '']\n",
    "})\n",
    "\n",
    "print(\"Messy data:\")\n",
    "print(messy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the problem to the LLM\n",
    "cleaning_request = \"\"\"\n",
    "I have CSV files where the 'concentration' column sometimes has:\n",
    "- Numbers like \"1.5\"\n",
    "- Text like \"1.5 mM\" or \"1.5mM\"\n",
    "- Missing values as \"N/A\", \"NA\", or empty\n",
    "\n",
    "Write a function called `clean_concentration` that:\n",
    "1. Takes a pandas Series as input\n",
    "2. Extracts the numeric value (assuming all values are in mM)\n",
    "3. Returns NaN for missing/invalid values\n",
    "4. Returns a cleaned pandas Series\n",
    "\n",
    "Include examples in the docstring.\n",
    "\"\"\"\n",
    "\n",
    "code = ask_llm(AUTOMATION_SYSTEM_PROMPT, cleaning_request, temperature=0.3)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute and test the generated function\n",
    "success, local_vars, error = safe_execute(code)\n",
    "\n",
    "if success:\n",
    "    # The function should now be available\n",
    "    print(\"Original concentration values:\")\n",
    "    print(messy_df['concentration'].tolist())\n",
    "    print(\"\\nCleaned values:\")\n",
    "    print(clean_concentration(messy_df['concentration']).tolist())\n",
    "else:\n",
    "    print(f\"Error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. File Processing Workflows\n",
    "\n",
    "### 3.1 Creating Sample Files\n",
    "\n",
    "Let's create some sample files to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory with sample experiment files\n",
    "import os\n",
    "\n",
    "# Create directory\n",
    "os.makedirs('sample_experiments', exist_ok=True)\n",
    "\n",
    "# Generate sample experiment files with different formats\n",
    "experiments = [\n",
    "    {\n",
    "        'filename': 'sample_experiments/exp_2024_01.csv',\n",
    "        'data': pd.DataFrame({\n",
    "            'SubjectID': ['M001', 'M002', 'M003', 'M004', 'M005'],\n",
    "            'Treatment': ['Drug_A', 'Drug_A', 'Placebo', 'Drug_A', 'Placebo'],\n",
    "            'Measurement': [45.2, 52.1, 32.4, 48.7, 33.8],\n",
    "            'Date': ['2024-01-15']*5\n",
    "        })\n",
    "    },\n",
    "    {\n",
    "        'filename': 'sample_experiments/exp_2024_02.csv',\n",
    "        'data': pd.DataFrame({\n",
    "            'subject_id': ['M006', 'M007', 'M008', 'M009', 'M010'],\n",
    "            'treatment': ['Drug_B', 'Placebo', 'Drug_B', 'Drug_B', 'Placebo'],\n",
    "            'measurement_value': [61.2, 31.5, 58.9, 62.4, 34.1],\n",
    "            'exp_date': ['2024-02-10']*5\n",
    "        })\n",
    "    },\n",
    "    {\n",
    "        'filename': 'sample_experiments/exp_2024_03.csv',\n",
    "        'data': pd.DataFrame({\n",
    "            'ID': ['M011', 'M012', 'M013', 'M014', 'M015'],\n",
    "            'Trt': ['Drug_A', 'Drug_B', 'Control', 'Drug_A', 'Drug_B'],\n",
    "            'Value': [47.3, 59.8, 32.9, 51.2, 63.1],\n",
    "            'Date_Collected': ['2024-03-05']*5\n",
    "        })\n",
    "    }\n",
    "]\n",
    "\n",
    "for exp in experiments:\n",
    "    exp['data'].to_csv(exp['filename'], index=False)\n",
    "    print(f\"Created: {exp['filename']}\")\n",
    "    print(exp['data'].head(2))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Batch Processing Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the LLM what we're working with\n",
    "file_descriptions = \"\"\"\n",
    "I have 3 CSV files from different experiments. They have similar data but different column names:\n",
    "\n",
    "File 1 (exp_2024_01.csv):\n",
    "- SubjectID, Treatment, Measurement, Date\n",
    "\n",
    "File 2 (exp_2024_02.csv):\n",
    "- subject_id, treatment, measurement_value, exp_date\n",
    "\n",
    "File 3 (exp_2024_03.csv):\n",
    "- ID, Trt, Value, Date_Collected\n",
    "- Note: 'Trt' values include 'Control' instead of 'Placebo'\n",
    "\n",
    "I want to combine them into a single DataFrame with standardized columns:\n",
    "- subject_id\n",
    "- treatment (with 'Control' renamed to 'Placebo')\n",
    "- measurement\n",
    "- date\n",
    "- source_file (which file the data came from)\n",
    "\n",
    "Write a function that:\n",
    "1. Takes a directory path as input\n",
    "2. Finds all CSV files\n",
    "3. Standardizes column names using the mapping I described\n",
    "4. Combines all files into one DataFrame\n",
    "5. Returns the combined DataFrame\n",
    "\"\"\"\n",
    "\n",
    "code = ask_llm(AUTOMATION_SYSTEM_PROMPT, file_descriptions, temperature=0.3)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute and test\n",
    "success, local_vars, error = safe_execute(code)\n",
    "\n",
    "if success:\n",
    "    # Try the function\n",
    "    combined_df = combine_experiment_files('sample_experiments')\n",
    "    print(f\"Combined {len(combined_df)} rows from experiment files:\")\n",
    "    print(combined_df)\n",
    "else:\n",
    "    print(f\"Error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Data Cleaning with AI\n",
    "\n",
    "### 4.1 Describe Messy Data, Get Cleaning Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create messy data with multiple issues\n",
    "messy_data = pd.DataFrame({\n",
    "    'sample_id': ['EXP001', 'EXP002', 'EXP003', 'EXP004', 'EXP005', \n",
    "                  'EXP006', 'EXP007', 'EXP008', 'EXP009', 'EXP010'],\n",
    "    'concentration': ['1.5', '2.0 mM', '1.5mM', 'N/A', '3.0', \n",
    "                      '2.5 mM', '1.0', 'NA', '1.75 mM', ''],\n",
    "    'measurement_date': ['2024-01-15', '01/16/2024', 'Jan 17, 2024', '2024-01-18', '2024/01/19',\n",
    "                         '20-Jan-2024', '2024-01-21', 'January 22, 2024', '2024-01-23', '01-24-2024'],\n",
    "    'temperature': ['25C', '25.0', '25 C', 'NA', '25.5°C', \n",
    "                    '', '25C', '25.0 C', '25', 'N/A'],\n",
    "    'cell_count': ['1.2e6', '1500000', '1.1E6', '1.3e6', '1250000',\n",
    "                   '1.4e6', 'N/A', '1.35E6', '1.28e6', '1320000'],\n",
    "    'status': ['Complete', 'complete', 'COMPLETE', 'Failed', 'Complete',\n",
    "               'complete', 'Complete', 'COMPLETE', 'Incomplete', 'Complete']\n",
    "})\n",
    "\n",
    "print(\"Messy data sample:\")\n",
    "print(messy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask LLM to analyze and clean the data\n",
    "cleaning_prompt = f\"\"\"\n",
    "Analyze this messy data and write a comprehensive cleaning function:\n",
    "\n",
    "{messy_data.to_string()}\n",
    "\n",
    "Issues I've noticed:\n",
    "1. concentration: mixed formats (plain numbers, with units, missing values)\n",
    "2. measurement_date: many different date formats\n",
    "3. temperature: mixed formats (with/without units, missing values)\n",
    "4. cell_count: scientific notation mixed with regular numbers, missing values\n",
    "5. status: inconsistent capitalization\n",
    "\n",
    "Write a function `clean_experiment_data(df)` that:\n",
    "1. Cleans each column appropriately\n",
    "2. Converts dates to datetime\n",
    "3. Converts numeric columns to float\n",
    "4. Standardizes status to title case\n",
    "5. Returns the cleaned DataFrame\n",
    "\n",
    "Include a summary of what was cleaned at the end.\n",
    "\"\"\"\n",
    "\n",
    "code = ask_llm(AUTOMATION_SYSTEM_PROMPT, cleaning_prompt, temperature=0.3)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute and test\n",
    "success, local_vars, error = safe_execute(code)\n",
    "\n",
    "if success:\n",
    "    cleaned_df = clean_experiment_data(messy_data)\n",
    "    print(\"\\nCleaned data:\")\n",
    "    print(cleaned_df)\n",
    "    print(\"\\nData types:\")\n",
    "    print(cleaned_df.dtypes)\n",
    "else:\n",
    "    print(f\"Error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Practical Exercises\n",
    "\n",
    "### Exercise A: Clean Inconsistent Date Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dates in various formats\n",
    "dates_df = pd.DataFrame({\n",
    "    'entry': range(1, 11),\n",
    "    'recorded_date': [\n",
    "        '2024-01-15',\n",
    "        '01/16/2024', \n",
    "        'Jan 17, 2024',\n",
    "        '18-01-2024',\n",
    "        '2024/01/19',\n",
    "        '20 January 2024',\n",
    "        '1/21/24',\n",
    "        '22-Jan-2024',\n",
    "        'January 23rd, 2024',\n",
    "        '24.01.2024'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Dates to clean:\")\n",
    "print(dates_df)\n",
    "\n",
    "# YOUR CODE HERE: Write a prompt to create a date cleaning function\n",
    "# date_cleaning_prompt = \"...\"\n",
    "# code = ask_llm(AUTOMATION_SYSTEM_PROMPT, date_cleaning_prompt, temperature=0.3)\n",
    "# print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise B: Extract Numerical Values from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab notes with embedded numerical values\n",
    "lab_notes_df = pd.DataFrame({\n",
    "    'experiment': ['EXP001', 'EXP002', 'EXP003', 'EXP004', 'EXP005'],\n",
    "    'notes': [\n",
    "        'Measured pH of 7.2, temperature stable at 37C',\n",
    "        'Final concentration: 2.5 mM. Yield was 89%.',\n",
    "        'Cell viability 95.3%, counted 1.2e6 cells',\n",
    "        'OD600 reading: 0.85. Growth rate 0.42/hr.',\n",
    "        'Protein concentration 150 ug/mL (Bradford assay)'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Lab notes to parse:\")\n",
    "print(lab_notes_df)\n",
    "\n",
    "# YOUR CODE HERE: Write a prompt to extract numerical values\n",
    "# The output should have columns for each type of measurement found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise C: Merge Files with Different Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create files with different schemas but overlapping data\n",
    "os.makedirs('merge_exercise', exist_ok=True)\n",
    "\n",
    "# File 1: Subject demographics\n",
    "demographics = pd.DataFrame({\n",
    "    'participant_id': ['P001', 'P002', 'P003', 'P004', 'P005'],\n",
    "    'age': [28, 34, 45, 31, 52],\n",
    "    'gender': ['F', 'M', 'F', 'M', 'F'],\n",
    "    'group': ['Treatment', 'Control', 'Treatment', 'Control', 'Treatment']\n",
    "})\n",
    "demographics.to_csv('merge_exercise/demographics.csv', index=False)\n",
    "\n",
    "# File 2: Baseline measurements\n",
    "baseline = pd.DataFrame({\n",
    "    'SubjectID': ['P001', 'P002', 'P003', 'P004', 'P005'],\n",
    "    'BaselineScore': [72, 68, 81, 75, 69],\n",
    "    'BaselineDate': ['2024-01-01']*5\n",
    "})\n",
    "baseline.to_csv('merge_exercise/baseline_scores.csv', index=False)\n",
    "\n",
    "# File 3: Follow-up measurements\n",
    "followup = pd.DataFrame({\n",
    "    'ID': ['P001', 'P002', 'P003', 'P004', 'P005'],\n",
    "    'Score_Week4': [78, 70, 89, 74, 82],\n",
    "    'Score_Week8': [82, 69, 94, 76, 88],\n",
    "    'Completed': [True, True, True, False, True]\n",
    "})\n",
    "followup.to_csv('merge_exercise/followup.csv', index=False)\n",
    "\n",
    "print(\"Files created. Schema overview:\")\n",
    "print(\"\\ndemographics.csv:\", demographics.columns.tolist())\n",
    "print(\"baseline_scores.csv:\", baseline.columns.tolist())\n",
    "print(\"followup.csv:\", followup.columns.tolist())\n",
    "\n",
    "# YOUR CODE HERE: Write a prompt to merge these files\n",
    "# Hint: The subject ID column has different names in each file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Building Your Script Library\n",
    "\n",
    "### 6.1 Saving Functions for Reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_function_to_file(code, filename, module_docstring=None):\n",
    "    \"\"\"\n",
    "    Save generated code to a Python file for reuse.\n",
    "    \n",
    "    Args:\n",
    "        code: Generated Python code\n",
    "        filename: Output file name (e.g., 'cleaning_utils.py')\n",
    "        module_docstring: Optional docstring for the module\n",
    "    \"\"\"\n",
    "    # Clean the code\n",
    "    clean_code = code.strip()\n",
    "    if clean_code.startswith('```python'):\n",
    "        clean_code = clean_code[9:]\n",
    "    if clean_code.startswith('```'):\n",
    "        clean_code = clean_code[3:]\n",
    "    if clean_code.endswith('```'):\n",
    "        clean_code = clean_code[:-3]\n",
    "    clean_code = clean_code.strip()\n",
    "    \n",
    "    # Add imports and docstring\n",
    "    header = '\"\"\"\\n' + (module_docstring or 'Auto-generated utility functions.') + '\\n\"\"\"\\n\\n'\n",
    "    header += 'import pandas as pd\\nimport numpy as np\\nimport re\\nfrom datetime import datetime\\n\\n'\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(header + clean_code)\n",
    "    \n",
    "    print(f\"Saved to {filename}\")\n",
    "\n",
    "# Example: Save a cleaning function\n",
    "# save_function_to_file(code, 'my_cleaning_utils.py', 'Data cleaning utilities for my lab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Generate Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_docstring(code):\n",
    "    \"\"\"\n",
    "    Ask LLM to add or improve docstrings in code.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Add comprehensive docstrings to this code.\n",
    "Include:\n",
    "- Function description\n",
    "- Args with types and descriptions\n",
    "- Returns description\n",
    "- Example usage\n",
    "- Any important notes or warnings\n",
    "\n",
    "Code:\n",
    "{code}\n",
    "\n",
    "Output only the improved code with docstrings.\"\"\"\n",
    "    \n",
    "    return ask_llm(AUTOMATION_SYSTEM_PROMPT, prompt, temperature=0.3)\n",
    "\n",
    "# Example: Improve documentation on generated code\n",
    "simple_function = \"\"\"\n",
    "def normalize_column(series):\n",
    "    return (series - series.min()) / (series.max() - series.min())\n",
    "\"\"\"\n",
    "\n",
    "documented = add_docstring(simple_function)\n",
    "print(documented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Advanced: Pipeline Generator\n",
    "\n",
    "Create a complete data processing pipeline from a description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pipeline(description, input_example=None):\n",
    "    \"\"\"\n",
    "    Generate a complete data processing pipeline from a description.\n",
    "    \n",
    "    Args:\n",
    "        description: Natural language description of the pipeline\n",
    "        input_example: Optional sample input data (as string)\n",
    "    \"\"\"\n",
    "    pipeline_prompt = f\"\"\"Create a complete data processing pipeline based on this description:\n",
    "\n",
    "{description}\n",
    "\n",
    "\"\"\"\n",
    "    if input_example:\n",
    "        pipeline_prompt += f\"\\nExample input data:\\n{input_example}\\n\"\n",
    "    \n",
    "    pipeline_prompt += \"\"\"\n",
    "Create a class called DataPipeline with:\n",
    "1. __init__ method that sets up any configuration\n",
    "2. Individual methods for each processing step\n",
    "3. A run() method that executes all steps in order\n",
    "4. A save_results() method to export the final data\n",
    "5. Logging at each step to show progress\n",
    "\n",
    "Include example usage at the end.\n",
    "\"\"\"\n",
    "    \n",
    "    return ask_llm(AUTOMATION_SYSTEM_PROMPT, pipeline_prompt, temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a pipeline\n",
    "pipeline_description = \"\"\"\n",
    "I need a pipeline to process gene expression data that:\n",
    "1. Loads a CSV file with gene names in the first column and sample values in other columns\n",
    "2. Filters out genes with low variance (bottom 20%)\n",
    "3. Log2-transforms the expression values (adding 1 to avoid log of zero)\n",
    "4. Z-score normalizes each gene across samples\n",
    "5. Identifies the top 50 most variable genes\n",
    "6. Saves the processed data to a new CSV file\n",
    "\"\"\"\n",
    "\n",
    "pipeline_code = generate_pipeline(pipeline_description)\n",
    "print(pipeline_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample gene expression data to test\n",
    "np.random.seed(42)\n",
    "n_genes = 100\n",
    "n_samples = 6\n",
    "\n",
    "gene_data = pd.DataFrame({\n",
    "    'gene_name': [f'Gene_{i:03d}' for i in range(n_genes)]\n",
    "})\n",
    "\n",
    "for i in range(n_samples):\n",
    "    gene_data[f'Sample_{i+1}'] = np.random.exponential(scale=100, size=n_genes)\n",
    "\n",
    "gene_data.to_csv('gene_expression.csv', index=False)\n",
    "print(\"Sample gene expression data:\")\n",
    "print(gene_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute and test the pipeline\n",
    "success, local_vars, error = safe_execute(pipeline_code)\n",
    "\n",
    "if success:\n",
    "    print(\"Pipeline created successfully!\")\n",
    "    # Try to run it\n",
    "    try:\n",
    "        pipeline = DataPipeline('gene_expression.csv')\n",
    "        result = pipeline.run()\n",
    "        print(\"\\nPipeline output:\")\n",
    "        print(result.head())\n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline execution error: {e}\")\n",
    "else:\n",
    "    print(f\"Error creating pipeline: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Key Takeaways\n",
    "\n",
    "### What You've Learned:\n",
    "1. **Show and Automate**: Describe manual processes to get automated functions\n",
    "2. **Batch Processing**: Handle multiple files with different formats\n",
    "3. **Data Cleaning**: Let AI write cleaning code from problem descriptions\n",
    "4. **Pipeline Generation**: Create complete processing workflows\n",
    "\n",
    "### Best Practices:\n",
    "- **Be specific** about edge cases and data quirks\n",
    "- **Include examples** of messy data in your prompts\n",
    "- **Test generated code** on sample data before using on real data\n",
    "- **Save useful functions** for reuse across projects\n",
    "- **Add documentation** to generated code\n",
    "\n",
    "### Prompt Patterns for Automation:\n",
    "\n",
    "| Task | Prompt Pattern |\n",
    "|------|----------------|\n",
    "| Clean column | \"Column has [formats]. Write function to extract [what you need].\" |\n",
    "| Merge files | \"Files have columns [list]. Create unified schema with [columns].\" |\n",
    "| Batch process | \"For each file in [dir], do [action] and combine results.\" |\n",
    "| Pipeline | \"Create pipeline that: 1) [step], 2) [step], ... n) [step].\" |\n",
    "\n",
    "---\n",
    "\n",
    "## Next: Notebook 4 - Your First AI Agent\n",
    "\n",
    "In the next notebook, you'll learn to:\n",
    "- Understand what agents are and when to use them\n",
    "- Build a simple research assistant agent\n",
    "- Chain multiple LLM calls for complex tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup (optional)\n",
    "import shutil\n",
    "# Uncomment to clean up created directories\n",
    "# shutil.rmtree('sample_experiments', ignore_errors=True)\n",
    "# shutil.rmtree('merge_exercise', ignore_errors=True)\n",
    "# os.remove('gene_expression.csv') if os.path.exists('gene_expression.csv') else None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
