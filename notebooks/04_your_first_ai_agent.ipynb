{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: Your First AI Agent\n",
    "\n",
    "## AI4Science series: Programming with Large Language Models\n",
    "\n",
    "**Duration**: ~40 minutes\n",
    "\n",
    "**Learning Goals**:\n",
    "- Understand what agents are and when to use them\n",
    "- Build a simple research assistant agent\n",
    "- Chain multiple LLM calls for complex tasks\n",
    "- Create practical agents for scientific workflows\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai pandas numpy matplotlib seaborn scipy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from typing import Callable, Dict, List, Any\n",
    "\n",
    "# API setup\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    api_key = userdata.get('OPENAI_API_KEY')\n",
    "except:\n",
    "    api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"Please set your OPENAI_API_KEY\")\n",
    "\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def ask_llm(system_prompt, user_message, model=\"gpt-4o-mini\", temperature=0.7):\n",
    "    \"\"\"Simple wrapper for OpenAI API calls.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. What is an Agent?\n",
    "\n",
    "### The Key Formula:\n",
    "\n",
    "$$\\text{Agent} = \\text{LLM} + \\text{Tools} + \\text{Loop}$$\n",
    "\n",
    "| Component | What It Does | Example |\n",
    "|-----------|--------------|--------|\n",
    "| **LLM** | Thinks and decides | \"I need to analyze this data\" |\n",
    "| **Tools** | Takes actions | `load_csv()`, `calculate_stats()`, `create_plot()` |\n",
    "| **Loop** | Iterates until done | Keep working until the task is complete |\n",
    "\n",
    "### When to Use Agents vs. Single Prompts:\n",
    "\n",
    "| Use Single Prompt | Use Agent |\n",
    "|-------------------|----------|\n",
    "| One-step tasks | Multi-step tasks |\n",
    "| Fixed input â†’ output | Dynamic decisions needed |\n",
    "| \"Summarize this text\" | \"Analyze my data and tell me what's interesting\" |\n",
    "| \"Convert this format\" | \"Research this topic and write a summary\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ReAct Pattern: Reasoning + Acting\n",
    "\n",
    "Agents follow a think-act-observe cycle:\n",
    "\n",
    "1. **Thought**: \"I need to understand the data first\"\n",
    "2. **Action**: Call `load_data()` tool\n",
    "3. **Observation**: \"Data has 100 rows, columns: x, y, z\"\n",
    "4. **Thought**: \"Now I should look for correlations\"\n",
    "5. **Action**: Call `calculate_correlation()` tool\n",
    "6. ... continue until task is complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Building Blocks: Tools\n",
    "\n",
    "Tools are Python functions that the agent can call. Let's build some research-relevant tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for our agent to work with\n",
    "np.random.seed(42)\n",
    "\n",
    "# Experiment data\n",
    "n_subjects = 60\n",
    "experiment_data = pd.DataFrame({\n",
    "    'subject_id': [f'S{i:03d}' for i in range(1, n_subjects + 1)],\n",
    "    'treatment': np.repeat(['Drug_A', 'Drug_B', 'Placebo'], n_subjects // 3),\n",
    "    'age': np.random.randint(25, 65, n_subjects),\n",
    "    'baseline_score': np.random.normal(50, 10, n_subjects),\n",
    "    'week_4_score': np.concatenate([\n",
    "        np.random.normal(58, 8, n_subjects // 3),   # Drug_A\n",
    "        np.random.normal(65, 9, n_subjects // 3),   # Drug_B\n",
    "        np.random.normal(52, 10, n_subjects // 3),  # Placebo\n",
    "    ]),\n",
    "    'adverse_events': np.random.poisson(1, n_subjects)\n",
    "})\n",
    "\n",
    "experiment_data['improvement'] = experiment_data['week_4_score'] - experiment_data['baseline_score']\n",
    "\n",
    "# Save for agent to use\n",
    "experiment_data.to_csv('clinical_trial_data.csv', index=False)\n",
    "print(\"Sample clinical trial data created:\")\n",
    "print(experiment_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools as Python functions\n",
    "\n",
    "def load_csv(filepath: str) -> str:\n",
    "    \"\"\"\n",
    "    Load a CSV file and return a summary.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the CSV file\n",
    "        \n",
    "    Returns:\n",
    "        Summary string with shape, columns, and sample data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        summary = f\"\"\"Loaded {filepath}:\n",
    "- Shape: {df.shape[0]} rows, {df.shape[1]} columns\n",
    "- Columns: {', '.join(df.columns.tolist())}\n",
    "- Data types:\\n{df.dtypes.to_string()}\n",
    "- First 3 rows:\\n{df.head(3).to_string()}\"\"\"\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        return f\"Error loading {filepath}: {str(e)}\"\n",
    "\n",
    "\n",
    "def compute_statistics(filepath: str, column: str, group_by: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Compute descriptive statistics for a column.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to CSV file\n",
    "        column: Column to analyze\n",
    "        group_by: Optional column to group by\n",
    "        \n",
    "    Returns:\n",
    "        Statistics as a formatted string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        \n",
    "        if group_by:\n",
    "            stats_df = df.groupby(group_by)[column].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "            return f\"Statistics for {column} by {group_by}:\\n{stats_df.to_string()}\"\n",
    "        else:\n",
    "            series = df[column]\n",
    "            return f\"\"\"Statistics for {column}:\n",
    "- Count: {series.count()}\n",
    "- Mean: {series.mean():.2f}\n",
    "- Std: {series.std():.2f}\n",
    "- Min: {series.min():.2f}\n",
    "- Max: {series.max():.2f}\n",
    "- Median: {series.median():.2f}\"\"\"\n",
    "    except Exception as e:\n",
    "        return f\"Error computing statistics: {str(e)}\"\n",
    "\n",
    "\n",
    "def run_statistical_test(filepath: str, column: str, group_column: str, test_type: str = \"anova\") -> str:\n",
    "    \"\"\"\n",
    "    Run a statistical test comparing groups.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to CSV file\n",
    "        column: Dependent variable column\n",
    "        group_column: Grouping variable column\n",
    "        test_type: Type of test (\"anova\", \"ttest\", \"kruskal\")\n",
    "        \n",
    "    Returns:\n",
    "        Test results as formatted string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        groups = [group[column].values for name, group in df.groupby(group_column)]\n",
    "        group_names = df[group_column].unique().tolist()\n",
    "        \n",
    "        if test_type == \"anova\":\n",
    "            stat, pvalue = stats.f_oneway(*groups)\n",
    "            test_name = \"One-way ANOVA\"\n",
    "        elif test_type == \"kruskal\":\n",
    "            stat, pvalue = stats.kruskal(*groups)\n",
    "            test_name = \"Kruskal-Wallis H-test\"\n",
    "        elif test_type == \"ttest\" and len(groups) == 2:\n",
    "            stat, pvalue = stats.ttest_ind(groups[0], groups[1])\n",
    "            test_name = \"Independent t-test\"\n",
    "        else:\n",
    "            return \"Invalid test type or group count for selected test\"\n",
    "        \n",
    "        significance = \"significant\" if pvalue < 0.05 else \"not significant\"\n",
    "        \n",
    "        return f\"\"\"{test_name} results for {column} by {group_column}:\n",
    "- Groups compared: {group_names}\n",
    "- Test statistic: {stat:.4f}\n",
    "- P-value: {pvalue:.4f}\n",
    "- Conclusion: The difference is {significance} at Î±=0.05\"\"\"\n",
    "    except Exception as e:\n",
    "        return f\"Error running test: {str(e)}\"\n",
    "\n",
    "\n",
    "def create_visualization(filepath: str, plot_type: str, x: str, y: str = None, hue: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Create a visualization and save it.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to CSV file\n",
    "        plot_type: Type of plot (\"boxplot\", \"barplot\", \"scatter\", \"histogram\")\n",
    "        x: X-axis column\n",
    "        y: Y-axis column (for some plot types)\n",
    "        hue: Grouping column for color\n",
    "        \n",
    "    Returns:\n",
    "        Status message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        if plot_type == \"boxplot\":\n",
    "            sns.boxplot(data=df, x=x, y=y, hue=hue)\n",
    "            plt.title(f\"Box Plot: {y} by {x}\")\n",
    "        elif plot_type == \"barplot\":\n",
    "            sns.barplot(data=df, x=x, y=y, hue=hue, errorbar='se')\n",
    "            plt.title(f\"Bar Plot: {y} by {x}\")\n",
    "        elif plot_type == \"scatter\":\n",
    "            sns.scatterplot(data=df, x=x, y=y, hue=hue)\n",
    "            plt.title(f\"Scatter Plot: {y} vs {x}\")\n",
    "        elif plot_type == \"histogram\":\n",
    "            sns.histplot(data=df, x=x, hue=hue, kde=True)\n",
    "            plt.title(f\"Histogram: {x}\")\n",
    "        else:\n",
    "            return f\"Unknown plot type: {plot_type}\"\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        output_file = f\"{plot_type}_{x}_{y or 'dist'}.png\"\n",
    "        plt.savefig(output_file, dpi=150)\n",
    "        plt.show()\n",
    "        \n",
    "        return f\"Created {plot_type} visualization and saved to {output_file}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error creating visualization: {str(e)}\"\n",
    "\n",
    "\n",
    "def explain_results(context: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Use LLM to explain statistical results in plain language.\n",
    "    \n",
    "    Args:\n",
    "        context: Statistical results or data summary\n",
    "        question: What to explain\n",
    "        \n",
    "    Returns:\n",
    "        Plain language explanation\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Based on these results:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a clear, plain-language explanation that a non-statistician could understand.\n",
    "Focus on practical implications and what this means for the research.\"\"\"\n",
    "    \n",
    "    return ask_llm(\n",
    "        \"You are a helpful statistics explainer for scientists.\",\n",
    "        prompt,\n",
    "        temperature=0.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our tools\n",
    "print(\"=== Testing load_csv ===\")\n",
    "print(load_csv('clinical_trial_data.csv'))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"=== Testing compute_statistics ===\")\n",
    "print(compute_statistics('clinical_trial_data.csv', 'improvement', 'treatment'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Your First Agent: Research Assistant\n",
    "\n",
    "Now let's build an agent that can use these tools to analyze data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent:\n",
    "    \"\"\"\n",
    "    A simple agent that can use tools to accomplish tasks.\n",
    "    Uses the ReAct pattern: Reasoning + Acting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tools: Dict[str, Callable], max_iterations: int = 10):\n",
    "        \"\"\"\n",
    "        Initialize the agent with available tools.\n",
    "        \n",
    "        Args:\n",
    "            tools: Dictionary mapping tool names to functions\n",
    "            max_iterations: Maximum number of think-act cycles\n",
    "        \"\"\"\n",
    "        self.tools = tools\n",
    "        self.max_iterations = max_iterations\n",
    "        self.history = []  # Track conversation history\n",
    "        \n",
    "        # Build tool descriptions for the prompt\n",
    "        self.tool_descriptions = self._build_tool_descriptions()\n",
    "        \n",
    "    def _build_tool_descriptions(self) -> str:\n",
    "        \"\"\"Create descriptions of available tools for the system prompt.\"\"\"\n",
    "        descriptions = []\n",
    "        for name, func in self.tools.items():\n",
    "            doc = func.__doc__ or \"No description available\"\n",
    "            descriptions.append(f\"- {name}: {doc.split(chr(10))[0].strip()}\")\n",
    "        return \"\\n\".join(descriptions)\n",
    "    \n",
    "    def _get_system_prompt(self) -> str:\n",
    "        \"\"\"Build the system prompt for the agent.\"\"\"\n",
    "        return f\"\"\"You are a research data analysis agent. You help scientists analyze their data.\n",
    "\n",
    "You have access to these tools:\n",
    "{self.tool_descriptions}\n",
    "\n",
    "To use a tool, respond with:\n",
    "THOUGHT: [your reasoning about what to do next]\n",
    "ACTION: [tool_name]\n",
    "PARAMETERS: [JSON object with parameters]\n",
    "\n",
    "After receiving a tool result, continue with more THOUGHT/ACTION pairs until the task is complete.\n",
    "\n",
    "When you have completed the task, respond with:\n",
    "THOUGHT: [summary of what you found]\n",
    "FINAL_ANSWER: [your complete response to the user]\n",
    "\n",
    "Important rules:\n",
    "1. Always think before acting\n",
    "2. Use tools to gather information - don't make up data\n",
    "3. If a tool fails, try a different approach\n",
    "4. Provide clear, actionable insights\n",
    "\"\"\"\n",
    "    \n",
    "    def _parse_response(self, response: str) -> Dict:\n",
    "        \"\"\"Parse the LLM response to extract thought, action, and parameters.\"\"\"\n",
    "        result = {\n",
    "            'thought': None,\n",
    "            'action': None,\n",
    "            'parameters': None,\n",
    "            'final_answer': None\n",
    "        }\n",
    "        \n",
    "        lines = response.strip().split('\\n')\n",
    "        current_section = None\n",
    "        current_content = []\n",
    "        \n",
    "        for line in lines:\n",
    "            if line.startswith('THOUGHT:'):\n",
    "                if current_section == 'parameters':\n",
    "                    result['parameters'] = '\\n'.join(current_content).strip()\n",
    "                current_section = 'thought'\n",
    "                current_content = [line[8:].strip()]\n",
    "            elif line.startswith('ACTION:'):\n",
    "                if current_section == 'thought':\n",
    "                    result['thought'] = '\\n'.join(current_content).strip()\n",
    "                current_section = 'action'\n",
    "                result['action'] = line[7:].strip()\n",
    "            elif line.startswith('PARAMETERS:'):\n",
    "                current_section = 'parameters'\n",
    "                current_content = [line[11:].strip()]\n",
    "            elif line.startswith('FINAL_ANSWER:'):\n",
    "                if current_section == 'thought':\n",
    "                    result['thought'] = '\\n'.join(current_content).strip()\n",
    "                current_section = 'final'\n",
    "                current_content = [line[13:].strip()]\n",
    "            else:\n",
    "                current_content.append(line)\n",
    "        \n",
    "        # Capture final section\n",
    "        if current_section == 'parameters':\n",
    "            result['parameters'] = '\\n'.join(current_content).strip()\n",
    "        elif current_section == 'final':\n",
    "            result['final_answer'] = '\\n'.join(current_content).strip()\n",
    "        elif current_section == 'thought' and not result['action']:\n",
    "            result['thought'] = '\\n'.join(current_content).strip()\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def _execute_tool(self, action: str, parameters: str) -> str:\n",
    "        \"\"\"Execute a tool with given parameters.\"\"\"\n",
    "        if action not in self.tools:\n",
    "            return f\"Error: Unknown tool '{action}'. Available tools: {list(self.tools.keys())}\"\n",
    "        \n",
    "        try:\n",
    "            # Parse parameters as JSON\n",
    "            params = json.loads(parameters) if parameters else {}\n",
    "            result = self.tools[action](**params)\n",
    "            return str(result)\n",
    "        except json.JSONDecodeError:\n",
    "            return f\"Error: Could not parse parameters as JSON: {parameters}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error executing {action}: {str(e)}\"\n",
    "    \n",
    "    def run(self, task: str, verbose: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Run the agent on a task.\n",
    "        \n",
    "        Args:\n",
    "            task: The task description from the user\n",
    "            verbose: Whether to print intermediate steps\n",
    "            \n",
    "        Returns:\n",
    "            The final answer from the agent\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self._get_system_prompt()},\n",
    "            {\"role\": \"user\", \"content\": f\"Task: {task}\"}\n",
    "        ]\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            if verbose:\n",
    "                print(f\"\\n{'='*50}\")\n",
    "                print(f\"Iteration {iteration + 1}\")\n",
    "                print('='*50)\n",
    "            \n",
    "            # Get LLM response\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=messages,\n",
    "                temperature=0.3\n",
    "            )\n",
    "            \n",
    "            assistant_message = response.choices[0].message.content\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "            \n",
    "            # Parse the response\n",
    "            parsed = self._parse_response(assistant_message)\n",
    "            \n",
    "            if verbose and parsed['thought']:\n",
    "                print(f\"\\nTHOUGHT: {parsed['thought']}\")\n",
    "            \n",
    "            # Check if we have a final answer\n",
    "            if parsed['final_answer']:\n",
    "                if verbose:\n",
    "                    print(f\"\\nFINAL ANSWER: {parsed['final_answer']}\")\n",
    "                return parsed['final_answer']\n",
    "            \n",
    "            # Execute the action if we have one\n",
    "            if parsed['action']:\n",
    "                if verbose:\n",
    "                    print(f\"ACTION: {parsed['action']}\")\n",
    "                    print(f\"PARAMETERS: {parsed['parameters']}\")\n",
    "                \n",
    "                result = self._execute_tool(parsed['action'], parsed['parameters'])\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"\\nOBSERVATION:\\n{result[:500]}{'...' if len(result) > 500 else ''}\")\n",
    "                \n",
    "                # Add the observation to messages\n",
    "                messages.append({\"role\": \"user\", \"content\": f\"OBSERVATION:\\n{result}\"})\n",
    "            else:\n",
    "                # No action and no final answer - ask for clarification\n",
    "                messages.append({\"role\": \"user\", \"content\": \"Please continue with your analysis. Use a tool or provide your final answer.\"})\n",
    "        \n",
    "        return \"Agent reached maximum iterations without completing the task.\"\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the agent's history.\"\"\"\n",
    "        self.history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our agent with the research tools\n",
    "research_tools = {\n",
    "    'load_csv': load_csv,\n",
    "    'compute_statistics': compute_statistics,\n",
    "    'run_statistical_test': run_statistical_test,\n",
    "    'create_visualization': create_visualization,\n",
    "    'explain_results': explain_results\n",
    "}\n",
    "\n",
    "agent = SimpleAgent(research_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent on a task\n",
    "task = \"\"\"\n",
    "I have a clinical trial dataset in 'clinical_trial_data.csv'.\n",
    "Please:\n",
    "1. Load the data and tell me what's in it\n",
    "2. Compare the improvement scores across treatment groups\n",
    "3. Run a statistical test to see if the differences are significant\n",
    "4. Create a visualization showing the results\n",
    "5. Explain what this means for my research\n",
    "\"\"\"\n",
    "\n",
    "result = agent.run(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. A Practical Agent: Data Analysis Assistant\n",
    "\n",
    "Let's create a more focused agent specifically for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAnalysisAgent:\n",
    "    \"\"\"\n",
    "    A specialized agent for exploratory data analysis.\n",
    "    Automatically explores data and finds interesting patterns.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filepath: str):\n",
    "        self.filepath = filepath\n",
    "        self.df = pd.read_csv(filepath)\n",
    "        self.findings = []\n",
    "        \n",
    "    def explore(self) -> str:\n",
    "        \"\"\"Automatically explore the dataset and generate insights.\"\"\"\n",
    "        insights = []\n",
    "        \n",
    "        # Basic info\n",
    "        insights.append(f\"Dataset Overview: {self.df.shape[0]} rows, {self.df.shape[1]} columns\")\n",
    "        \n",
    "        # Identify column types\n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_cols = self.df.select_dtypes(include=['object']).columns.tolist()\n",
    "        \n",
    "        insights.append(f\"Numeric columns: {numeric_cols}\")\n",
    "        insights.append(f\"Categorical columns: {categorical_cols}\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing = self.df.isnull().sum()\n",
    "        if missing.any():\n",
    "            insights.append(f\"Missing values: {missing[missing > 0].to_dict()}\")\n",
    "        else:\n",
    "            insights.append(\"No missing values found\")\n",
    "        \n",
    "        return \"\\n\".join(insights)\n",
    "    \n",
    "    def find_correlations(self, threshold: float = 0.5) -> str:\n",
    "        \"\"\"Find strong correlations between numeric variables.\"\"\"\n",
    "        numeric_df = self.df.select_dtypes(include=[np.number])\n",
    "        \n",
    "        if len(numeric_df.columns) < 2:\n",
    "            return \"Not enough numeric columns for correlation analysis\"\n",
    "        \n",
    "        corr_matrix = numeric_df.corr()\n",
    "        \n",
    "        # Find strong correlations\n",
    "        strong_corrs = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                corr = corr_matrix.iloc[i, j]\n",
    "                if abs(corr) >= threshold:\n",
    "                    strong_corrs.append({\n",
    "                        'var1': corr_matrix.columns[i],\n",
    "                        'var2': corr_matrix.columns[j],\n",
    "                        'correlation': corr\n",
    "                    })\n",
    "        \n",
    "        if strong_corrs:\n",
    "            result = f\"Strong correlations found (threshold={threshold}):\\n\"\n",
    "            for c in strong_corrs:\n",
    "                result += f\"  - {c['var1']} vs {c['var2']}: r={c['correlation']:.3f}\\n\"\n",
    "            return result\n",
    "        else:\n",
    "            return f\"No correlations found above threshold {threshold}\"\n",
    "    \n",
    "    def compare_groups(self, value_col: str, group_col: str) -> str:\n",
    "        \"\"\"Compare a numeric variable across groups.\"\"\"\n",
    "        groups = self.df.groupby(group_col)[value_col].agg(['count', 'mean', 'std'])\n",
    "        \n",
    "        # Run ANOVA\n",
    "        group_data = [group[value_col].values for name, group in self.df.groupby(group_col)]\n",
    "        f_stat, p_value = stats.f_oneway(*group_data)\n",
    "        \n",
    "        result = f\"Comparison of {value_col} by {group_col}:\\n\"\n",
    "        result += groups.to_string() + \"\\n\\n\"\n",
    "        result += f\"ANOVA: F={f_stat:.2f}, p={p_value:.4f}\\n\"\n",
    "        result += f\"Significant difference: {'Yes' if p_value < 0.05 else 'No'}\"\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def visualize_comparison(self, value_col: str, group_col: str):\n",
    "        \"\"\"Create a comparison visualization.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Box plot\n",
    "        sns.boxplot(data=self.df, x=group_col, y=value_col, ax=axes[0])\n",
    "        axes[0].set_title(f'{value_col} by {group_col}')\n",
    "        \n",
    "        # Bar plot with error bars\n",
    "        sns.barplot(data=self.df, x=group_col, y=value_col, errorbar='se', ax=axes[1])\n",
    "        axes[1].set_title(f'Mean {value_col} (Â±SE)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def auto_analyze(self) -> str:\n",
    "        \"\"\"Run automatic analysis and return summary.\"\"\"\n",
    "        report = [\"=\" * 50]\n",
    "        report.append(\"AUTOMATED DATA ANALYSIS REPORT\")\n",
    "        report.append(\"=\" * 50)\n",
    "        \n",
    "        # Exploration\n",
    "        report.append(\"\\n1. DATA OVERVIEW\")\n",
    "        report.append(self.explore())\n",
    "        \n",
    "        # Correlations\n",
    "        report.append(\"\\n2. CORRELATION ANALYSIS\")\n",
    "        report.append(self.find_correlations())\n",
    "        \n",
    "        # Group comparisons (if categorical columns exist)\n",
    "        categorical_cols = self.df.select_dtypes(include=['object']).columns.tolist()\n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        if categorical_cols and numeric_cols:\n",
    "            report.append(\"\\n3. GROUP COMPARISONS\")\n",
    "            for cat_col in categorical_cols[:2]:  # Limit to first 2\n",
    "                for num_col in numeric_cols[:3]:  # Limit to first 3\n",
    "                    if self.df[cat_col].nunique() <= 5:  # Only if reasonable number of groups\n",
    "                        report.append(f\"\\n--- {num_col} by {cat_col} ---\")\n",
    "                        report.append(self.compare_groups(num_col, cat_col))\n",
    "        \n",
    "        # Generate interpretation\n",
    "        report.append(\"\\n4. KEY FINDINGS\")\n",
    "        interpretation = ask_llm(\n",
    "            \"You are a data analyst. Summarize the key findings from this analysis in 3-4 bullet points.\",\n",
    "            \"\\n\".join(report),\n",
    "            temperature=0.5\n",
    "        )\n",
    "        report.append(interpretation)\n",
    "        \n",
    "        return \"\\n\".join(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Data Analysis Agent\n",
    "analyst = DataAnalysisAgent('clinical_trial_data.csv')\n",
    "report = analyst.auto_analyze()\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visualization\n",
    "analyst.visualize_comparison('improvement', 'treatment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Practical Exercises\n",
    "\n",
    "### Exercise A: Build a Methods Section Writer Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def methods_writer_tool(analysis_description: str, field: str = \"biomedical\") -> str:\n",
    "    \"\"\"\n",
    "    Generate a methods section based on analysis description.\n",
    "    \n",
    "    Args:\n",
    "        analysis_description: Description of the analysis performed\n",
    "        field: Research field (for appropriate style)\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Write a methods section for a {field} research paper based on this analysis:\n",
    "\n",
    "{analysis_description}\n",
    "\n",
    "Include:\n",
    "1. Statistical methods used\n",
    "2. Software/tools used (assume Python with pandas, scipy, matplotlib)\n",
    "3. Significance thresholds\n",
    "4. Any assumptions made\n",
    "\n",
    "Write in formal academic style, past tense, third person.\"\"\"\n",
    "    \n",
    "    return ask_llm(\n",
    "        \"You are a scientific writing expert who writes clear, precise methods sections.\",\n",
    "        prompt,\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "# YOUR CODE HERE: Create a simple agent that:\n",
    "# 1. Analyzes data\n",
    "# 2. Generates a methods section based on the analysis\n",
    "\n",
    "# Example usage:\n",
    "# methods = methods_writer_tool(report, \"biomedical\")\n",
    "# print(methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise B: Build a Statistical Test Selector Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_statistical_test(data_description: str, research_question: str) -> str:\n",
    "    \"\"\"\n",
    "    Suggest appropriate statistical tests based on data and question.\n",
    "    \n",
    "    Args:\n",
    "        data_description: Description of the data\n",
    "        research_question: The question to answer\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Based on this data and research question, recommend the appropriate statistical test(s):\n",
    "\n",
    "DATA:\n",
    "{data_description}\n",
    "\n",
    "RESEARCH QUESTION:\n",
    "{research_question}\n",
    "\n",
    "Please provide:\n",
    "1. Recommended test(s)\n",
    "2. Why this test is appropriate\n",
    "3. Assumptions to check\n",
    "4. Alternative tests if assumptions are violated\n",
    "5. Python code to run the test\"\"\"\n",
    "    \n",
    "    return ask_llm(\n",
    "        \"You are a biostatistics expert who helps researchers choose appropriate statistical methods.\",\n",
    "        prompt,\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "# YOUR CODE HERE: Test this tool with your own data and question\n",
    "# data_desc = analyst.explore()\n",
    "# question = \"Is there a significant difference in improvement between treatment groups?\"\n",
    "# recommendation = suggest_statistical_test(data_desc, question)\n",
    "# print(recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise C: Build a Figure Caption Generator Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_figure_caption(figure_description: str, statistical_results: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Generate a publication-ready figure caption.\n",
    "    \n",
    "    Args:\n",
    "        figure_description: What the figure shows\n",
    "        statistical_results: Optional statistical test results to include\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Write a publication-ready figure caption for this figure:\n",
    "\n",
    "FIGURE DESCRIPTION:\n",
    "{figure_description}\n",
    "\"\"\"\n",
    "    if statistical_results:\n",
    "        prompt += f\"\\nSTATISTICAL RESULTS TO INCLUDE:\\n{statistical_results}\\n\"\n",
    "    \n",
    "    prompt += \"\"\"\n",
    "The caption should:\n",
    "1. Start with a brief title (e.g., \"Figure 1. Treatment effects on improvement scores.\")\n",
    "2. Describe what is shown (plot type, variables)\n",
    "3. Explain visual elements (error bars, significance markers, etc.)\n",
    "4. Include sample sizes and statistical results if provided\n",
    "5. Be concise but complete\n",
    "\"\"\"\n",
    "    \n",
    "    return ask_llm(\n",
    "        \"You are a scientific writing expert who writes precise figure captions.\",\n",
    "        prompt,\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "# YOUR CODE HERE: Generate a caption for the visualization we created\n",
    "# figure_desc = \"Box plot and bar chart comparing improvement scores across three treatment groups (Drug_A, Drug_B, Placebo)\"\n",
    "# stats_results = \"ANOVA: F=XX, p=XX. Drug_B showed significantly higher improvement than Placebo.\"\n",
    "# caption = generate_figure_caption(figure_desc, stats_results)\n",
    "# print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Key Takeaways\n",
    "\n",
    "### What You've Learned:\n",
    "1. **Agent Formula**: LLM + Tools + Loop\n",
    "2. **ReAct Pattern**: Think â†’ Act â†’ Observe â†’ Repeat\n",
    "3. **Tool Design**: Creating useful, well-documented functions\n",
    "4. **Practical Applications**: Data analysis, statistics, writing assistance\n",
    "\n",
    "### When to Use Agents:\n",
    "\n",
    "| Good for Agents | Better with Single Prompts |\n",
    "|----------------|---------------------------|\n",
    "| Exploratory analysis | Fixed transformations |\n",
    "| Multi-step workflows | Simple Q&A |\n",
    "| Tasks needing decisions | Template filling |\n",
    "| Dynamic problem-solving | Deterministic tasks |\n",
    "\n",
    "### Best Practices:\n",
    "- **Start simple**: Build basic agents before complex ones\n",
    "- **Good tools**: Well-designed tools make agents more reliable\n",
    "- **Clear prompts**: Tell the agent exactly what format to use\n",
    "- **Iteration limits**: Always set a maximum to prevent infinite loops\n",
    "- **Error handling**: Make tools robust to unexpected inputs\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with combining different tools\n",
    "- Build agents for your specific research workflows\n",
    "- Explore more advanced agent frameworks (LangChain, AutoGen)\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Workshop Summary\n",
    "\n",
    "Congratulations! You've completed the \"LLM Allies for Scientists\" workshop.\n",
    "\n",
    "### What You've Learned Across All Notebooks:\n",
    "\n",
    "| Notebook | Key Skills |\n",
    "|----------|------------|\n",
    "| 1. First AI Assistant | API basics, prompt engineering, literature analysis |\n",
    "| 2. Data Visualization | Natural language â†’ code, iterative refinement |\n",
    "| 3. Automation | Script generation, batch processing, data cleaning |\n",
    "| 4. AI Agents | Multi-step reasoning, tool use, autonomous analysis |\n",
    "\n",
    "### Keep Exploring:\n",
    "- Try these techniques on your own research data\n",
    "- Build custom tools for your specific domain\n",
    "- Share useful prompts with your lab colleagues\n",
    "- Stay updated on new AI capabilities\n",
    "\n",
    "Happy researching! ðŸ”¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import os\n",
    "# Uncomment to remove generated files\n",
    "# os.remove('clinical_trial_data.csv') if os.path.exists('clinical_trial_data.csv') else None\n",
    "# for f in os.listdir('.'):\n",
    "#     if f.endswith('.png'):\n",
    "#         os.remove(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
